{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, wandb, torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from monai import transforms, data\n",
    "from monai.data import DataLoader, DistributedSampler\n",
    "from monai.utils import set_determinism\n",
    "from tqdm import tqdm\n",
    "import torch.distributed as dist\n",
    "import torch\n",
    "from torch import nn\n",
    "from diffusers import AutoencoderKLWan, WanTransformer3DModel\n",
    "from peft import LoraConfig, inject_adapter_in_model\n",
    "from flow_match import FlowMatchScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_json_path = './json/train.json'\n",
    "with open(train_json_path) as f:\n",
    "    train_files = json.load(f)\n",
    "val_json_path = './json/val.json'\n",
    "with open(val_json_path) as f:\n",
    "    val_files = json.load(f)\n",
    "train_batchsize  = 1\n",
    "\n",
    "transforms_1mm = transforms.Compose(\n",
    "    [transforms.Spacingd(keys=[\"image\"], pixdim=(1, 1, 1), mode=(\"bilinear\")),\n",
    "    transforms.Spacingd(keys=[\"brainmask\"], pixdim=(1, 1, 1), mode=(\"nearest\")),\n",
    "    transforms.SpatialPadd(keys=[\"image\",\"brainmask\"], spatial_size=(160, 160, 128)),\n",
    "    transforms.CropForegroundd(keys=[\"image\"], source_key=\"brainmask\",allow_smaller=False),\n",
    "    transforms.DeleteItemsd(keys=[\"brainmask\"]),\n",
    "    transforms.RandSpatialCropd(keys=[\"image\"], roi_size=(80, 80, 64),max_roi_size = (100, 100, 80), random_size=True),\n",
    "    transforms.Resized(keys=[\"image\"], spatial_size=(80, 80, 64), size_mode = 'all', mode='bilinear'),\n",
    "    ]\n",
    ")\n",
    "transforms_2mm = transforms.Compose(\n",
    "    [transforms.Spacingd(keys=[\"image\"], pixdim=(2, 2, 2), mode=(\"bilinear\")),\n",
    "    transforms.Spacingd(keys=[\"brainmask\"], pixdim=(2, 2, 2), mode=(\"nearest\")),\n",
    "    transforms.SpatialPadd(keys=[\"image\",\"brainmask\"], spatial_size=(80, 80, 64)),\n",
    "    transforms.CropForegroundd(keys=[\"image\"], source_key=\"brainmask\",allow_smaller=False),\n",
    "    transforms.DeleteItemsd(keys=[\"brainmask\"]),\n",
    "    transforms.Resized(keys=[\"image\"], spatial_size=80, size_mode = 'longest', mode='bilinear'),\n",
    "    transforms.CenterSpatialCropd(keys=[\"image\"], roi_size=(80, 80, 64)),\n",
    "    transforms.SpatialPadd(keys=[\"image\"], spatial_size=(80, 80, 64)),\n",
    "    ]\n",
    ")\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.CopyItemsd(keys=[\"image\"], names=[\"path\"]),\n",
    "        transforms.LoadImaged(keys=[\"image\",\"brainmask\"]),\n",
    "        transforms.EnsureChannelFirstd(keys=[\"image\",\"brainmask\"]),\n",
    "        transforms.EnsureTyped(keys=[\"image\",\"brainmask\"]),\n",
    "        transforms.Orientationd(keys=[\"image\",\"brainmask\"], axcodes=\"RAS\"),\n",
    "        transforms.RandAffined(\n",
    "            keys=[\"image\",\"brainmask\"],\n",
    "            rotate_range=(-np.pi / 36, np.pi / 36),\n",
    "            translate_range=(-1, 1),\n",
    "            scale_range=(-0.05, 0.05),\n",
    "            padding_mode=\"zeros\",\n",
    "            prob=0.5,\n",
    "        ),\n",
    "        transforms.CropForegroundd(\n",
    "            keys=[\"image\", \"brainmask\"],\n",
    "            source_key=\"brainmask\",\n",
    "            allow_smaller=False,\n",
    "        ),\n",
    "        transforms.ResizeWithPadOrCropd(keys=[\"image\",\"brainmask\"], spatial_size=(192, 192, 141)),\n",
    "        transforms.ScaleIntensityRangePercentilesd(keys=\"image\", lower=0.5, upper=99.5, b_min=0, b_max=1,clip=True ),\n",
    "    ]\n",
    ")\n",
    "train_ds = data.Dataset(data=train_files, transform=train_transforms)\n",
    "# sampler_train = DistributedSampler(train_ds, num_replicas=4, rank=rank)\n",
    "train_loader = DataLoader(train_ds, batch_size=train_batchsize, shuffle=False, num_workers=8, persistent_workers=True, drop_last=True, sampler=None)\n",
    "\n",
    "val_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.CopyItemsd(keys=[\"image\"], names=[\"path\"]),\n",
    "        transforms.LoadImaged(keys=[\"image\",\"brainmask\"]),\n",
    "        transforms.EnsureChannelFirstd(keys=[\"image\",\"brainmask\"]),\n",
    "        transforms.EnsureTyped(keys=[\"image\",\"brainmask\"]),\n",
    "        transforms.Orientationd(keys=[\"image\",\"brainmask\"], axcodes=\"RAS\"),\n",
    "        transforms.CropForegroundd(\n",
    "            keys=[\"image\", \"brainmask\"],\n",
    "            source_key=\"brainmask\",\n",
    "            allow_smaller=False,\n",
    "        ),\n",
    "        transforms.ResizeWithPadOrCropd(keys=[\"image\",\"brainmask\"], spatial_size=(192, 192, 141)),\n",
    "        transforms.ScaleIntensityRangePercentilesd(keys=\"image\", lower=0.5, upper=99.5, b_min=0, b_max=1),\n",
    "    ]\n",
    ")\n",
    "val_ds = data.Dataset(data=val_files, transform=train_transforms)\n",
    "# sampler_val = DistributedSampler(val_ds, num_replicas=4, rank=rank)\n",
    "val_loader = DataLoader(val_ds, batch_size=train_batchsize, shuffle=False, num_workers=8, persistent_workers=True, drop_last=True, sampler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoencoderKLWan(\n",
       "  (encoder): WanEncoder3d(\n",
       "    (nonlinearity): SiLU()\n",
       "    (conv_in): WanCausalConv3d(3, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "    (down_blocks): ModuleList(\n",
       "      (0-1): 2 x WanResidualBlock(\n",
       "        (nonlinearity): SiLU()\n",
       "        (norm1): WanRMS_norm()\n",
       "        (conv1): WanCausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "        (norm2): WanRMS_norm()\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): WanCausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "        (conv_shortcut): Identity()\n",
       "      )\n",
       "      (2): WanResample(\n",
       "        (resample): Sequential(\n",
       "          (0): ZeroPad2d((0, 1, 0, 1))\n",
       "          (1): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2))\n",
       "        )\n",
       "      )\n",
       "      (3): WanResidualBlock(\n",
       "        (nonlinearity): SiLU()\n",
       "        (norm1): WanRMS_norm()\n",
       "        (conv1): WanCausalConv3d(96, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "        (norm2): WanRMS_norm()\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): WanCausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "        (conv_shortcut): WanCausalConv3d(96, 192, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "      )\n",
       "      (4): WanResidualBlock(\n",
       "        (nonlinearity): SiLU()\n",
       "        (norm1): WanRMS_norm()\n",
       "        (conv1): WanCausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "        (norm2): WanRMS_norm()\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): WanCausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "        (conv_shortcut): Identity()\n",
       "      )\n",
       "      (5): WanResample(\n",
       "        (resample): Sequential(\n",
       "          (0): ZeroPad2d((0, 1, 0, 1))\n",
       "          (1): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2))\n",
       "        )\n",
       "        (time_conv): WanCausalConv3d(192, 192, kernel_size=(3, 1, 1), stride=(2, 1, 1))\n",
       "      )\n",
       "      (6): WanResidualBlock(\n",
       "        (nonlinearity): SiLU()\n",
       "        (norm1): WanRMS_norm()\n",
       "        (conv1): WanCausalConv3d(192, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "        (norm2): WanRMS_norm()\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): WanCausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "        (conv_shortcut): WanCausalConv3d(192, 384, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "      )\n",
       "      (7): WanResidualBlock(\n",
       "        (nonlinearity): SiLU()\n",
       "        (norm1): WanRMS_norm()\n",
       "        (conv1): WanCausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "        (norm2): WanRMS_norm()\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): WanCausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "        (conv_shortcut): Identity()\n",
       "      )\n",
       "      (8): WanResample(\n",
       "        (resample): Sequential(\n",
       "          (0): ZeroPad2d((0, 1, 0, 1))\n",
       "          (1): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2))\n",
       "        )\n",
       "        (time_conv): WanCausalConv3d(384, 384, kernel_size=(3, 1, 1), stride=(2, 1, 1))\n",
       "      )\n",
       "      (9-10): 2 x WanResidualBlock(\n",
       "        (nonlinearity): SiLU()\n",
       "        (norm1): WanRMS_norm()\n",
       "        (conv1): WanCausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "        (norm2): WanRMS_norm()\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): WanCausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "        (conv_shortcut): Identity()\n",
       "      )\n",
       "    )\n",
       "    (mid_block): WanMidBlock(\n",
       "      (attentions): ModuleList(\n",
       "        (0): WanAttentionBlock(\n",
       "          (norm): WanRMS_norm()\n",
       "          (to_qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (proj): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x WanResidualBlock(\n",
       "          (nonlinearity): SiLU()\n",
       "          (norm1): WanRMS_norm()\n",
       "          (conv1): WanCausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "          (norm2): WanRMS_norm()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): WanCausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "          (conv_shortcut): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm_out): WanRMS_norm()\n",
       "    (conv_out): WanCausalConv3d(384, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "  )\n",
       "  (quant_conv): WanCausalConv3d(32, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "  (post_quant_conv): WanCausalConv3d(16, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "  (decoder): WanDecoder3d(\n",
       "    (nonlinearity): SiLU()\n",
       "    (conv_in): WanCausalConv3d(16, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "    (mid_block): WanMidBlock(\n",
       "      (attentions): ModuleList(\n",
       "        (0): WanAttentionBlock(\n",
       "          (norm): WanRMS_norm()\n",
       "          (to_qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (proj): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x WanResidualBlock(\n",
       "          (nonlinearity): SiLU()\n",
       "          (norm1): WanRMS_norm()\n",
       "          (conv1): WanCausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "          (norm2): WanRMS_norm()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): WanCausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "          (conv_shortcut): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (up_blocks): ModuleList(\n",
       "      (0): WanUpBlock(\n",
       "        (resnets): ModuleList(\n",
       "          (0-2): 3 x WanResidualBlock(\n",
       "            (nonlinearity): SiLU()\n",
       "            (norm1): WanRMS_norm()\n",
       "            (conv1): WanCausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "            (norm2): WanRMS_norm()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): WanCausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "            (conv_shortcut): Identity()\n",
       "          )\n",
       "        )\n",
       "        (upsamplers): ModuleList(\n",
       "          (0): WanResample(\n",
       "            (resample): Sequential(\n",
       "              (0): WanUpsample(scale_factor=(2.0, 2.0), mode='nearest-exact')\n",
       "              (1): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (time_conv): WanCausalConv3d(384, 768, kernel_size=(3, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): WanUpBlock(\n",
       "        (resnets): ModuleList(\n",
       "          (0): WanResidualBlock(\n",
       "            (nonlinearity): SiLU()\n",
       "            (norm1): WanRMS_norm()\n",
       "            (conv1): WanCausalConv3d(192, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "            (norm2): WanRMS_norm()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): WanCausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "            (conv_shortcut): WanCausalConv3d(192, 384, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "          (1-2): 2 x WanResidualBlock(\n",
       "            (nonlinearity): SiLU()\n",
       "            (norm1): WanRMS_norm()\n",
       "            (conv1): WanCausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "            (norm2): WanRMS_norm()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): WanCausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "            (conv_shortcut): Identity()\n",
       "          )\n",
       "        )\n",
       "        (upsamplers): ModuleList(\n",
       "          (0): WanResample(\n",
       "            (resample): Sequential(\n",
       "              (0): WanUpsample(scale_factor=(2.0, 2.0), mode='nearest-exact')\n",
       "              (1): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (time_conv): WanCausalConv3d(384, 768, kernel_size=(3, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): WanUpBlock(\n",
       "        (resnets): ModuleList(\n",
       "          (0-2): 3 x WanResidualBlock(\n",
       "            (nonlinearity): SiLU()\n",
       "            (norm1): WanRMS_norm()\n",
       "            (conv1): WanCausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "            (norm2): WanRMS_norm()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): WanCausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "            (conv_shortcut): Identity()\n",
       "          )\n",
       "        )\n",
       "        (upsamplers): ModuleList(\n",
       "          (0): WanResample(\n",
       "            (resample): Sequential(\n",
       "              (0): WanUpsample(scale_factor=(2.0, 2.0), mode='nearest-exact')\n",
       "              (1): Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): WanUpBlock(\n",
       "        (resnets): ModuleList(\n",
       "          (0-2): 3 x WanResidualBlock(\n",
       "            (nonlinearity): SiLU()\n",
       "            (norm1): WanRMS_norm()\n",
       "            (conv1): WanCausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "            (norm2): WanRMS_norm()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): WanCausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "            (conv_shortcut): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm_out): WanRMS_norm()\n",
       "    (conv_out): WanCausalConv3d(96, 3, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_id = \"Wan-AI/Wan2.1-T2V-14B-Diffusers\"\n",
    "vae = AutoencoderKLWan.from_pretrained(model_id, subfolder=\"vae\", torch_dtype=torch.bfloat16, cache_dir=\"/working/cache/huggingface/hub\")\n",
    "vae = vae.to(\"cuda\")\n",
    "vae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnifiedFlowNet(nn.Module):\n",
    "    def __init__(self, latent_dim, input_channels = 4,  drop_prob=0.1, lora = True):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.input_channels = input_channels\n",
    "        self.model = self.init_model(lora)\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def add_lora_to_model(self, model, lora_rank=4, lora_alpha=4, lora_target_modules=\"q,k,v,o,ffn.0,ffn.2\", init_lora_weights=\"kaiming\", pretrained_lora_path=None, state_dict_converter=None):\n",
    "        # Add LoRA to UNet\n",
    "        \n",
    "        lora_alpha = lora_alpha\n",
    "        if init_lora_weights == \"kaiming\":\n",
    "            init_lora_weights = True\n",
    "            \n",
    "        lora_config = LoraConfig(\n",
    "            r=lora_rank,\n",
    "            lora_alpha=lora_alpha,\n",
    "            init_lora_weights=init_lora_weights,\n",
    "            target_modules=lora_target_modules.split(\",\"),\n",
    "        )\n",
    "        model = inject_adapter_in_model(lora_config, model)\n",
    "        for param in model.parameters():\n",
    "            # Upcast LoRA parameters into fp32\n",
    "            if param.requires_grad:\n",
    "                param.data = param.to(torch.float32)\n",
    "        return model\n",
    "        \n",
    "    def init_model(self, lora):\n",
    "        transformer = WanTransformer3DModel.from_pretrained(\"Wan-AI/Wan2.1-T2V-1.3B-Diffusers\", subfolder=\"transformer\", torch_dtype=torch.bfloat16, cache_dir=\"/working/cache/huggingface/hub\")\n",
    "        old_patch_embed = transformer.patch_embedding\n",
    "        new_patch_embed = nn.Conv3d(\n",
    "            in_channels=old_patch_embed.in_channels*8,               # 修改为新输入通道\n",
    "            out_channels=old_patch_embed.out_channels,\n",
    "            kernel_size=old_patch_embed.kernel_size,\n",
    "            stride=old_patch_embed.stride,\n",
    "            padding=old_patch_embed.padding\n",
    "        )\n",
    "        transformer.patch_embedding = new_patch_embed\n",
    "        old_proj_out = transformer.proj_out\n",
    "        new_proj_out = nn.Linear(\n",
    "            in_features=old_proj_out.in_features,\n",
    "            out_features=old_proj_out.out_features*4,            # 修改为新输出通道\n",
    "            bias=True\n",
    "        )\n",
    "        transformer.proj_out = new_proj_out\n",
    "        if lora:\n",
    "            transformer = self.add_lora_to_model(transformer, lora_rank=4, lora_alpha=4, lora_target_modules=\"to_q,to_k,to_v,to_out.0,linear_1,linear_2\", init_lora_weights=\"kaiming\", pretrained_lora_path=None, state_dict_converter=None)\n",
    "        return transformer\n",
    "    \n",
    "    def forward(self, z_t, timestep, encoder_hidden_states=None, z_c=None):\n",
    "        B, C, D, H, W = z_t.shape\n",
    "        if encoder_hidden_states is None:\n",
    "            encoder_hidden_states = torch.zeros([1,256,4096], device=z_t.device)\n",
    "        if z_c is None:\n",
    "            z_c_in = torch.zeros_like(z_t, device=z_t.device, dtype=z_t.dtype)\n",
    "        else:\n",
    "            # 以 drop_prob 随机丢弃\n",
    "            mask = (torch.rand(B,self.input_channels, device=z_t.device, dtype=z_t.dtype) < self.drop_prob).float()\n",
    "            mask = mask.unsqueeze(2)\n",
    "            mask = mask.repeat(1,1,self.latent_dim)\n",
    "            mask = mask.view(B,self.latent_dim*self.input_channels,1,1,1)\n",
    "            z_c_keep = z_c\n",
    "            z_c_zero = torch.zeros_like(z_c, device=z_t.device, dtype=z_t.dtype)\n",
    "            z_c_in = z_c_keep * (1-mask) + z_c_zero * mask\n",
    "\n",
    "        # 拼接输入\n",
    "        inp = torch.cat([z_t, z_c_in], dim=1)\n",
    "        v_pred = self.model(inp, timestep, encoder_hidden_states)\n",
    "        return v_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = FlowMatchScheduler(shift=5, sigma_min=0.0, extra_one_step=True)\n",
    "scheduler.set_timesteps(1000, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e125884c804022955e3c25d9068b3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b452d3a2e1264b31bf738a0e577a4421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "UnifiedFlowNet(\n",
       "  (model): WanTransformer3DModel(\n",
       "    (rope): WanRotaryPosEmbed()\n",
       "    (patch_embedding): Conv3d(128, 1536, kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
       "    (condition_embedder): WanTimeTextImageEmbedding(\n",
       "      (timesteps_proj): Timesteps()\n",
       "      (time_embedder): TimestepEmbedding(\n",
       "        (linear_1): lora.Linear(\n",
       "          (base_layer): Linear(in_features=256, out_features=1536, bias=True)\n",
       "          (lora_dropout): ModuleDict(\n",
       "            (default): Identity()\n",
       "          )\n",
       "          (lora_A): ModuleDict(\n",
       "            (default): Linear(in_features=256, out_features=4, bias=False)\n",
       "          )\n",
       "          (lora_B): ModuleDict(\n",
       "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (lora_embedding_A): ParameterDict()\n",
       "          (lora_embedding_B): ParameterDict()\n",
       "          (lora_magnitude_vector): ModuleDict()\n",
       "        )\n",
       "        (act): SiLU()\n",
       "        (linear_2): lora.Linear(\n",
       "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (lora_dropout): ModuleDict(\n",
       "            (default): Identity()\n",
       "          )\n",
       "          (lora_A): ModuleDict(\n",
       "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
       "          )\n",
       "          (lora_B): ModuleDict(\n",
       "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (lora_embedding_A): ParameterDict()\n",
       "          (lora_embedding_B): ParameterDict()\n",
       "          (lora_magnitude_vector): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "      (act_fn): SiLU()\n",
       "      (time_proj): Linear(in_features=1536, out_features=9216, bias=True)\n",
       "      (text_embedder): PixArtAlphaTextProjection(\n",
       "        (linear_1): lora.Linear(\n",
       "          (base_layer): Linear(in_features=4096, out_features=1536, bias=True)\n",
       "          (lora_dropout): ModuleDict(\n",
       "            (default): Identity()\n",
       "          )\n",
       "          (lora_A): ModuleDict(\n",
       "            (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "          )\n",
       "          (lora_B): ModuleDict(\n",
       "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (lora_embedding_A): ParameterDict()\n",
       "          (lora_embedding_B): ParameterDict()\n",
       "          (lora_magnitude_vector): ModuleDict()\n",
       "        )\n",
       "        (act_1): GELU(approximate='tanh')\n",
       "        (linear_2): lora.Linear(\n",
       "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (lora_dropout): ModuleDict(\n",
       "            (default): Identity()\n",
       "          )\n",
       "          (lora_A): ModuleDict(\n",
       "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
       "          )\n",
       "          (lora_B): ModuleDict(\n",
       "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (lora_embedding_A): ParameterDict()\n",
       "          (lora_embedding_B): ParameterDict()\n",
       "          (lora_magnitude_vector): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-29): 30 x WanTransformerBlock(\n",
       "        (norm1): FP32LayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (attn1): Attention(\n",
       "          (norm_q): RMSNorm()\n",
       "          (norm_k): RMSNorm()\n",
       "          (to_q): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Identity()\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=4, out_features=1536, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (to_k): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Identity()\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=4, out_features=1536, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (to_v): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Identity()\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=4, out_features=1536, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (to_out): ModuleList(\n",
       "            (0): lora.Linear(\n",
       "              (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1536, out_features=4, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=4, out_features=1536, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (attn2): Attention(\n",
       "          (norm_q): RMSNorm()\n",
       "          (norm_k): RMSNorm()\n",
       "          (to_q): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Identity()\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=4, out_features=1536, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (to_k): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Identity()\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=4, out_features=1536, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (to_v): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Identity()\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=4, out_features=1536, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (to_out): ModuleList(\n",
       "            (0): lora.Linear(\n",
       "              (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1536, out_features=4, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=4, out_features=1536, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): FP32LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (ffn): FeedForward(\n",
       "          (net): ModuleList(\n",
       "            (0): GELU(\n",
       "              (proj): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "            )\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm3): FP32LayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "      )\n",
       "    )\n",
       "    (norm_out): FP32LayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "    (proj_out): Linear(in_features=1536, out_features=256, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = UnifiedFlowNet(latent_dim=16, input_channels=4, drop_prob=0.2, lora=True)\n",
    "model = model.to(\"cuda\")\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype, but got Float and BFloat16",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m noisy_latents = scheduler.add_noise(latents, noise, timestep)\n\u001b[32m     19\u001b[39m training_target = scheduler.training_target(latents, noise, timestep)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m noise_pred = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoisy_latents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_c\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlatents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m loss = torch.nn.functional.mse_loss(noise_pred.float(), training_target.float())\n\u001b[32m     23\u001b[39m loss = loss * scheduler.training_weight(timestep)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/working/Project/StableDiffusion/Stable_diffusion/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/working/Project/StableDiffusion/Stable_diffusion/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 69\u001b[39m, in \u001b[36mUnifiedFlowNet.forward\u001b[39m\u001b[34m(self, z_t, timestep, encoder_hidden_states, z_c)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# 拼接输入\u001b[39;00m\n\u001b[32m     68\u001b[39m inp = torch.cat([z_t, z_c_in], dim=\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m v_pred = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m v_pred\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/working/Project/StableDiffusion/Stable_diffusion/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/working/Project/StableDiffusion/Stable_diffusion/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/working/Project/StableDiffusion/Stable_diffusion/.venv/lib/python3.12/site-packages/diffusers/models/transformers/transformer_wan.py:425\u001b[39m, in \u001b[36mWanTransformer3DModel.forward\u001b[39m\u001b[34m(self, hidden_states, timestep, encoder_hidden_states, encoder_hidden_states_image, return_dict, attention_kwargs)\u001b[39m\n\u001b[32m    422\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.patch_embedding(hidden_states)\n\u001b[32m    423\u001b[39m hidden_states = hidden_states.flatten(\u001b[32m2\u001b[39m).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m425\u001b[39m temb, timestep_proj, encoder_hidden_states, encoder_hidden_states_image = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcondition_embedder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_hidden_states_image\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    428\u001b[39m timestep_proj = timestep_proj.unflatten(\u001b[32m1\u001b[39m, (\u001b[32m6\u001b[39m, -\u001b[32m1\u001b[39m))\n\u001b[32m    430\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m encoder_hidden_states_image \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/working/Project/StableDiffusion/Stable_diffusion/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/working/Project/StableDiffusion/Stable_diffusion/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/working/Project/StableDiffusion/Stable_diffusion/.venv/lib/python3.12/site-packages/diffusers/models/transformers/transformer_wan.py:158\u001b[39m, in \u001b[36mWanTimeTextImageEmbedding.forward\u001b[39m\u001b[34m(self, timestep, encoder_hidden_states, encoder_hidden_states_image)\u001b[39m\n\u001b[32m    156\u001b[39m     timestep = timestep.to(time_embedder_dtype)\n\u001b[32m    157\u001b[39m temb = \u001b[38;5;28mself\u001b[39m.time_embedder(timestep).type_as(encoder_hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m timestep_proj = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtime_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    160\u001b[39m encoder_hidden_states = \u001b[38;5;28mself\u001b[39m.text_embedder(encoder_hidden_states)\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m encoder_hidden_states_image \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/working/Project/StableDiffusion/Stable_diffusion/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/working/Project/StableDiffusion/Stable_diffusion/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/working/Project/StableDiffusion/Stable_diffusion/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 must have the same dtype, but got Float and BFloat16"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    image = batch['image']\n",
    "    mask = batch['brainmask']\n",
    "    path = batch['path']\n",
    "    image = image.permute(0, 1, 4, 2, 3)  # [B, C, H, W, D] -> [B, D, H, W, C]\n",
    "    B, C, D, H, W  = image.shape\n",
    "    image = image.view(B*C, D, H, W)  # [4, 1, D, H, W]\n",
    "    image = image.unsqueeze(1)\n",
    "    image = image.repeat(1, 3, 1, 1, 1)  # [4, 3, D, H, W]\n",
    "    image = image.to(torch.bfloat16).to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        encode = vae.encode(image, return_dict=True)\n",
    "    latent = encode.latent_dist.sample()\n",
    "    latents = latent.view(B, 64, -1, int(H/8), int(W/8))\n",
    "    noise = torch.randn_like(latents)\n",
    "    timestep_id = torch.randint(0, scheduler.num_train_timesteps, (1,))\n",
    "    timestep = scheduler.timesteps[timestep_id].to(dtype=latents.dtype, device=latents.device)\n",
    "    noisy_latents = scheduler.add_noise(latents, noise, timestep)\n",
    "    training_target = scheduler.training_target(latents, noise, timestep)\n",
    "\n",
    "    noise_pred = model(noisy_latents, timestep, encoder_hidden_states=None, z_c=latents)\n",
    "    loss = torch.nn.functional.mse_loss(noise_pred.float(), training_target.float())\n",
    "    loss = loss * scheduler.training_weight(timestep)\n",
    "    print(loss.item())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
