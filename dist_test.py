# dist_test.py
import os, torch, torch.distributed as dist

def main():
    # ----------------- åˆå§‹åŒ– -----------------
    dist.init_process_group(backend="nccl")          # GPU è®­ç»ƒé»˜è®¤ç”¨ NCCL
    rank        = dist.get_rank()                    # å…¨å±€ç¼–å·
    world_size  = dist.get_world_size()              # è¿›ç¨‹æ€»æ•°
    local_rank  = int(os.environ.get("LOCAL_RANK", 0))

    torch.cuda.set_device(local_rank)
    device = torch.device("cuda", local_rank)

    # ----------------- æ‰“å°åŸºæœ¬ä¿¡æ¯ -----------------
    if rank == 0:
        print(f"\nâœ“ Distributed initialized â€” world_size={world_size}\n")

    print(f"[Rank {rank:02d}] local_rank={local_rank}  device={device}")

    # ----------------- åšä¸€æ¬¡ all_reduce -----------------
    x = torch.ones(1, device=device) * rank          # tensor=[rank]
    dist.all_reduce(x, op=dist.ReduceOp.SUM)         # æ‰€æœ‰ rank æ±‚å’Œ
    expected = world_size * (world_size - 1) / 2     # 0+1+â€¦+(n-1)
    assert x.item() == expected, f"all_reduce failed: {x.item()} != {expected}"

    if rank == 0:
        print("\nâœ“ all_reduce result correct, basic comm OK ğŸ‰")

    dist.destroy_process_group()

if __name__ == "__main__":
    main()
    #CUDA_VISIBLE_DEVICES=2,3 torchrun --standalone --nproc_per_node=2 dist_test.py